{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "tf.get_logger().setLevel(logging.ERROR)\n",
    "\n",
    "EPOCHS = 32\n",
    "BATCH_SIZE = 256\n",
    "INPUT_FILE_NAME = '/content/drive/MyDrive/Software, AI, Distsys.../Deep Learning NVIDIA Book/frankenstein.txt'\n",
    "WINDOW_LENGTH = 40\n",
    "WINDOW_STEP = 3\n",
    "BEAM_SIZE = 8\n",
    "NUM_LETTERS = 11\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the input file\n",
    "file = open(INPUT_FILE_NAME, 'r', encoding='utf-8')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n",
    "# Make lowercase and remove newline and extra spaces\n",
    "text = text.lower()\n",
    "text = text.replace('\\n', ' ')\n",
    "text = text.replace(' ', ' ')\n",
    "\n",
    "# Encode characters as indices\n",
    "unique_chars = list(set(text))\n",
    "char_to_index = dict((ch, index) for index, ch in enumerate(unique_chars))\n",
    "index_to_char = dict((index, ch) for index, ch in enumerate(unique_chars))\n",
    "encoding_width = len(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training examples\n",
    "fragments = []\n",
    "targets = []\n",
    "for i in range(0, len(text) - WINDOW_LENGTH, WINDOW_STEP):\n",
    "  fragments.append(text[i: i + WINDOW_LENGTH])\n",
    "  targets.append(text[i + WINDOW_LENGTH])\n",
    "\n",
    "# Convert to one-hot encoded training data\n",
    "x = np.zeros((len(fragments), WINDOW_LENGTH, encoding_width))\n",
    "y = np.zeros((len(fragments), encoding_width))\n",
    "for i, fragment in enumerate(fragments):\n",
    "  for j, char in enumerate(fragment):\n",
    "    x[i, j, char_to_index[char]] = 1\n",
    "  target_char = targets[i]\n",
    "  y[i, char_to_index[target_char]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True,\n",
    "               dropout=0.2, recurrent_dropout=0.2,\n",
    "               input_shape=(None, encoding_width)))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(encoding_width, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()\n",
    "history = model.fit(x, y, validation_split=0.05,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS, verbose=2,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create initial single beam represented by triplet\n",
    "# (probablity, string , one-hot encoded string)\n",
    "letters = 'the body '\n",
    "one_hots = []\n",
    "for i, char in enumerate(letters):\n",
    "  x = np.zeros(encoding_width)\n",
    "  x[char_to_index[char]] = 1\n",
    "  one_hots.append(x)\n",
    "beams = [(np.log(1.0), letters, one_hots)]\n",
    "\n",
    "# Predict NUM_LETTERS into the future\n",
    "for i in range(NUM_LETTERS):\n",
    "  minibatch_list = []\n",
    "  # Create minibatch from one-hot encodings, and predict\n",
    "  for triple in beams:\n",
    "    minibatch_list.append(triple[2])\n",
    "  minibatch = np.array(minibatch_list)\n",
    "  y_predict = model.predict(minibatch, verbose=0)\n",
    "  new_beams = []\n",
    "  for j, softmax_vec in enumerate(y_predict):\n",
    "    triple = beams[j]\n",
    "    # Create BEAM_SIZE new beams from existing beam\n",
    "    for k in range(BEAM_SIZE):\n",
    "      char_index = np.argmax(softmax_vec)\n",
    "      new_prob = triple[0] + np.log(softmax_vec[char_index])\n",
    "      new_letters = triple[1] + index_to_char[char_index]\n",
    "      x = np.zeros(encoding_width)\n",
    "      x[char_index] = 1\n",
    "      new_one_hots = triple[2].copy()\n",
    "      new_one_hots.append(x)\n",
    "      new_beams.append((new_prob, new_letters, new_one_hots))\n",
    "      softmax_vec[char_index] = 0\n",
    "    # Prune tree to only keep BEAM_SIZE most probable beams\n",
    "    new_beams.sort(key=lambda tup: tup[0], reverse=True)\n",
    "    beams = new_beams[0:BEAM_SIZE]\n",
    "  for item in beams:\n",
    "    print(item[1])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
